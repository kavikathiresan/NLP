{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "XooFHFsI0SLq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the packages**"
      ],
      "metadata": {
        "id": "IZUpZO_Y2_9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\" Specific NLP processes like automatic summarization — analyzing a large volume of text data and producing an executive summary — will be a boon to many industries, including some that may not have been considered “big data industries” until now.\n",
        "\n",
        "And big data processes will, themselves, continue to benefit from improved NLP capabilities. So many data processes are about translating information from humans (language) to computers (data) for processing, and then translating it from computers (data) to humans (language) for analysis and decision making. As natural language processing continues to become more and more savvy, our big data capabilities can only become more and more sophisticated.\n",
        "\n",
        "Getting started with NLP and Talend\n",
        "NLP uses various analyses (lexical, syntactic, semantic, and pragmatic) to make it possible for computers to read, hear, and analyze language-based data. As a result, technologies such as chatbots are able to mimic human speech, and search engines are able to deliver more accurate results to users’ queries.\"\"\""
      ],
      "metadata": {
        "id": "rbEbSWkJ2bFh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjnE1mx8254A",
        "outputId": "3f206d56-3c95-4c00-b338-04f8c677055c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Specific NLP processes like automatic summarization — analyzing a large volume of text data and producing an executive summary — will be a boon to many industries, including some that may not have been considered “big data industries” until now.\n",
            "\n",
            "And big data processes will, themselves, continue to benefit from improved NLP capabilities. So many data processes are about translating information from humans (language) to computers (data) for processing, and then translating it from computers (data) to humans (language) for analysis and decision making. As natural language processing continues to become more and more savvy, our big data capabilities can only become more and more sophisticated.\n",
            "\n",
            "Getting started with NLP and Talend\n",
            "NLP uses various analyses (lexical, syntactic, semantic, and pragmatic) to make it possible for computers to read, hear, and analyze language-based data. As a result, technologies such as chatbots are able to mimic human speech, and search engines are able to deliver more accurate results to users’ queries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence and Word**"
      ],
      "metadata": {
        "id": "CR-cte2Q-exJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5mjVOI88zvA",
        "outputId": "6ac7d382-3b58-4444-98c1-85c69d9c7edd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "Total_sentence=sent_tokenize(text)\n",
        "print(f'no of total sentence:{len(Total_sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fafRLQyu-orO",
        "outputId": "b9996f83-74bc-4043-d14a-89eca4e496be"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of total sentence:6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "Total_word=word_tokenize(text)\n",
        "print(f'No of Total Words:{len(Total_word)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KwW5HOP_opp",
        "outputId": "a81b7d94-dec0-4b0e-b4d0-6b03503c0446"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of Total Words:191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STRING** **REMOVE**"
      ],
      "metadata": {
        "id": "5uDMcdoXBiKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "in8M9lkxBheh",
        "outputId": "31a9e275-67ad-4bea-f46a-293eef49c5b8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data=[]\n",
        "for i in text:\n",
        "  if i not in string.punctuation:\n",
        "    cleaned_data.append(i)\n",
        "cleaned_data=''.join(cleaned_data)\n",
        "print(cleaned_data)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xUbB57EBsyx",
        "outputId": "cf8cab97-4a3c-42a2-de4d-b7e2569be040"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Specific NLP processes like automatic summarization — analyzing a large volume of text data and producing an executive summary — will be a boon to many industries including some that may not have been considered “big data industries” until now\n",
            "\n",
            "And big data processes will themselves continue to benefit from improved NLP capabilities So many data processes are about translating information from humans language to computers data for processing and then translating it from computers data to humans language for analysis and decision making As natural language processing continues to become more and more savvy our big data capabilities can only become more and more sophisticated\n",
            "\n",
            "Getting started with NLP and Talend\n",
            "NLP uses various analyses lexical syntactic semantic and pragmatic to make it possible for computers to read hear and analyze languagebased data As a result technologies such as chatbots are able to mimic human speech and search engines are able to deliver more accurate results to users’ queries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_datas=cleaned_data.lower()"
      ],
      "metadata": {
        "id": "QHgkZkxsJLMm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STOPWORDS**"
      ],
      "metadata": {
        "id": "H1L1jNndEyzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slkiD1ioEuRm",
        "outputId": "6f4a341c-d64c-4fc7-a9af-496b7f243dba"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "KYvf5DD6HJO3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text=[]\n",
        "for i in cleaned_datas.split():\n",
        "  if i not in stopwords.words('english'):\n",
        "    clean_text.append(i)\n",
        "proper_text=' '.join(clean_text)\n",
        "print(proper_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0aAZWHSJczv",
        "outputId": "73cb13a1-e7d3-43f7-eaa2-ef36979bfe6b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "specific nlp processes like automatic summarization — analyzing large volume text data producing executive summary — boon many industries including may considered “big data industries” big data processes continue benefit improved nlp capabilities many data processes translating information humans language computers data processing translating computers data humans language analysis decision making natural language processing continues become savvy big data capabilities become sophisticated getting started nlp talend nlp uses various analyses lexical syntactic semantic pragmatic make possible computers read hear analyze languagebased data result technologies chatbots able mimic human speech search engines able deliver accurate results users’ queries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING AND LEMMAIZATION**"
      ],
      "metadata": {
        "id": "QUUbdaapK1it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3hqg-lWK6WT",
        "outputId": "14106df2-a751-45f7-8053-b44fb68f9dfd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming=PorterStemmer()\n",
        "for i in proper_text.split():\n",
        "  print('Before Stemming:',i,\"After stemming:\",stemming.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OGgL2BDLBvE",
        "outputId": "f5c9ff5e-f8ca-46cc-8b25-22bb94e24780"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming: specific After stemming: specif\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: processes After stemming: process\n",
            "Before Stemming: like After stemming: like\n",
            "Before Stemming: automatic After stemming: automat\n",
            "Before Stemming: summarization After stemming: summar\n",
            "Before Stemming: — After stemming: —\n",
            "Before Stemming: analyzing After stemming: analyz\n",
            "Before Stemming: large After stemming: larg\n",
            "Before Stemming: volume After stemming: volum\n",
            "Before Stemming: text After stemming: text\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: producing After stemming: produc\n",
            "Before Stemming: executive After stemming: execut\n",
            "Before Stemming: summary After stemming: summari\n",
            "Before Stemming: — After stemming: —\n",
            "Before Stemming: boon After stemming: boon\n",
            "Before Stemming: many After stemming: mani\n",
            "Before Stemming: industries After stemming: industri\n",
            "Before Stemming: including After stemming: includ\n",
            "Before Stemming: may After stemming: may\n",
            "Before Stemming: considered After stemming: consid\n",
            "Before Stemming: “big After stemming: “big\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: industries” After stemming: industries”\n",
            "Before Stemming: big After stemming: big\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: processes After stemming: process\n",
            "Before Stemming: continue After stemming: continu\n",
            "Before Stemming: benefit After stemming: benefit\n",
            "Before Stemming: improved After stemming: improv\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: capabilities After stemming: capabl\n",
            "Before Stemming: many After stemming: mani\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: processes After stemming: process\n",
            "Before Stemming: translating After stemming: translat\n",
            "Before Stemming: information After stemming: inform\n",
            "Before Stemming: humans After stemming: human\n",
            "Before Stemming: language After stemming: languag\n",
            "Before Stemming: computers After stemming: comput\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: processing After stemming: process\n",
            "Before Stemming: translating After stemming: translat\n",
            "Before Stemming: computers After stemming: comput\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: humans After stemming: human\n",
            "Before Stemming: language After stemming: languag\n",
            "Before Stemming: analysis After stemming: analysi\n",
            "Before Stemming: decision After stemming: decis\n",
            "Before Stemming: making After stemming: make\n",
            "Before Stemming: natural After stemming: natur\n",
            "Before Stemming: language After stemming: languag\n",
            "Before Stemming: processing After stemming: process\n",
            "Before Stemming: continues After stemming: continu\n",
            "Before Stemming: become After stemming: becom\n",
            "Before Stemming: savvy After stemming: savvi\n",
            "Before Stemming: big After stemming: big\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: capabilities After stemming: capabl\n",
            "Before Stemming: become After stemming: becom\n",
            "Before Stemming: sophisticated After stemming: sophist\n",
            "Before Stemming: getting After stemming: get\n",
            "Before Stemming: started After stemming: start\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: talend After stemming: talend\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: uses After stemming: use\n",
            "Before Stemming: various After stemming: variou\n",
            "Before Stemming: analyses After stemming: analys\n",
            "Before Stemming: lexical After stemming: lexic\n",
            "Before Stemming: syntactic After stemming: syntact\n",
            "Before Stemming: semantic After stemming: semant\n",
            "Before Stemming: pragmatic After stemming: pragmat\n",
            "Before Stemming: make After stemming: make\n",
            "Before Stemming: possible After stemming: possibl\n",
            "Before Stemming: computers After stemming: comput\n",
            "Before Stemming: read After stemming: read\n",
            "Before Stemming: hear After stemming: hear\n",
            "Before Stemming: analyze After stemming: analyz\n",
            "Before Stemming: languagebased After stemming: languagebas\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: result After stemming: result\n",
            "Before Stemming: technologies After stemming: technolog\n",
            "Before Stemming: chatbots After stemming: chatbot\n",
            "Before Stemming: able After stemming: abl\n",
            "Before Stemming: mimic After stemming: mimic\n",
            "Before Stemming: human After stemming: human\n",
            "Before Stemming: speech After stemming: speech\n",
            "Before Stemming: search After stemming: search\n",
            "Before Stemming: engines After stemming: engin\n",
            "Before Stemming: able After stemming: abl\n",
            "Before Stemming: deliver After stemming: deliv\n",
            "Before Stemming: accurate After stemming: accur\n",
            "Before Stemming: results After stemming: result\n",
            "Before Stemming: users’ After stemming: users’\n",
            "Before Stemming: queries After stemming: queri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ONESAUmqMDcV"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lemmatization=WordNetLemmatizer()\n",
        "for i in proper_text.split():\n",
        "  print('Before Stemming:',i,\"After stemming:\",lemmatization.lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlEfrHJbMChX",
        "outputId": "9b91b34c-b00b-4313-e086-1a290b4c5a97"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming: specific After stemming: specific\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: processes After stemming: process\n",
            "Before Stemming: like After stemming: like\n",
            "Before Stemming: automatic After stemming: automatic\n",
            "Before Stemming: summarization After stemming: summarization\n",
            "Before Stemming: — After stemming: —\n",
            "Before Stemming: analyzing After stemming: analyzing\n",
            "Before Stemming: large After stemming: large\n",
            "Before Stemming: volume After stemming: volume\n",
            "Before Stemming: text After stemming: text\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: producing After stemming: producing\n",
            "Before Stemming: executive After stemming: executive\n",
            "Before Stemming: summary After stemming: summary\n",
            "Before Stemming: — After stemming: —\n",
            "Before Stemming: boon After stemming: boon\n",
            "Before Stemming: many After stemming: many\n",
            "Before Stemming: industries After stemming: industry\n",
            "Before Stemming: including After stemming: including\n",
            "Before Stemming: may After stemming: may\n",
            "Before Stemming: considered After stemming: considered\n",
            "Before Stemming: “big After stemming: “big\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: industries” After stemming: industries”\n",
            "Before Stemming: big After stemming: big\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: processes After stemming: process\n",
            "Before Stemming: continue After stemming: continue\n",
            "Before Stemming: benefit After stemming: benefit\n",
            "Before Stemming: improved After stemming: improved\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: capabilities After stemming: capability\n",
            "Before Stemming: many After stemming: many\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: processes After stemming: process\n",
            "Before Stemming: translating After stemming: translating\n",
            "Before Stemming: information After stemming: information\n",
            "Before Stemming: humans After stemming: human\n",
            "Before Stemming: language After stemming: language\n",
            "Before Stemming: computers After stemming: computer\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: processing After stemming: processing\n",
            "Before Stemming: translating After stemming: translating\n",
            "Before Stemming: computers After stemming: computer\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: humans After stemming: human\n",
            "Before Stemming: language After stemming: language\n",
            "Before Stemming: analysis After stemming: analysis\n",
            "Before Stemming: decision After stemming: decision\n",
            "Before Stemming: making After stemming: making\n",
            "Before Stemming: natural After stemming: natural\n",
            "Before Stemming: language After stemming: language\n",
            "Before Stemming: processing After stemming: processing\n",
            "Before Stemming: continues After stemming: continues\n",
            "Before Stemming: become After stemming: become\n",
            "Before Stemming: savvy After stemming: savvy\n",
            "Before Stemming: big After stemming: big\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: capabilities After stemming: capability\n",
            "Before Stemming: become After stemming: become\n",
            "Before Stemming: sophisticated After stemming: sophisticated\n",
            "Before Stemming: getting After stemming: getting\n",
            "Before Stemming: started After stemming: started\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: talend After stemming: talend\n",
            "Before Stemming: nlp After stemming: nlp\n",
            "Before Stemming: uses After stemming: us\n",
            "Before Stemming: various After stemming: various\n",
            "Before Stemming: analyses After stemming: analysis\n",
            "Before Stemming: lexical After stemming: lexical\n",
            "Before Stemming: syntactic After stemming: syntactic\n",
            "Before Stemming: semantic After stemming: semantic\n",
            "Before Stemming: pragmatic After stemming: pragmatic\n",
            "Before Stemming: make After stemming: make\n",
            "Before Stemming: possible After stemming: possible\n",
            "Before Stemming: computers After stemming: computer\n",
            "Before Stemming: read After stemming: read\n",
            "Before Stemming: hear After stemming: hear\n",
            "Before Stemming: analyze After stemming: analyze\n",
            "Before Stemming: languagebased After stemming: languagebased\n",
            "Before Stemming: data After stemming: data\n",
            "Before Stemming: result After stemming: result\n",
            "Before Stemming: technologies After stemming: technology\n",
            "Before Stemming: chatbots After stemming: chatbots\n",
            "Before Stemming: able After stemming: able\n",
            "Before Stemming: mimic After stemming: mimic\n",
            "Before Stemming: human After stemming: human\n",
            "Before Stemming: speech After stemming: speech\n",
            "Before Stemming: search After stemming: search\n",
            "Before Stemming: engines After stemming: engine\n",
            "Before Stemming: able After stemming: able\n",
            "Before Stemming: deliver After stemming: deliver\n",
            "Before Stemming: accurate After stemming: accurate\n",
            "Before Stemming: results After stemming: result\n",
            "Before Stemming: users’ After stemming: users’\n",
            "Before Stemming: queries After stemming: query\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatization_text=[]\n",
        "lemmatization=WordNetLemmatizer()\n",
        "for i in proper_text.split():\n",
        "  lemmatization_text.append(lemmatization.lemmatize(i))\n",
        "lemmatization_texts=' '.join(lemmatization_text)\n",
        "print(lemmatization_texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehEkgjqdMwBV",
        "outputId": "24ae32da-f43c-415c-d1a8-49a0d3817a3a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "specific nlp process like automatic summarization — analyzing large volume text data producing executive summary — boon many industry including may considered “big data industries” big data process continue benefit improved nlp capability many data process translating information human language computer data processing translating computer data human language analysis decision making natural language processing continues become savvy big data capability become sophisticated getting started nlp talend nlp us various analysis lexical syntactic semantic pragmatic make possible computer read hear analyze languagebased data result technology chatbots able mimic human speech search engine able deliver accurate result users’ query\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert TEXT to Numbers**"
      ],
      "metadata": {
        "id": "PNS6qTakNaMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "8f3hOkBJNiF2"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=CountVectorizer()\n",
        "bag_words=b.fit_transform([lemmatization_texts])\n",
        "print(bag_words.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csDfsFy3OBFj",
        "outputId": "00d3f607-9761-48ad-f736-ba865e62492f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 1 2 1 1 1 2 1 3 1 2 1 3 1 1 1 8 1 1 1 1 1 1 3 1 1 1 1 1 3 1 1 1 1 1 1\n",
            "  2 1 1 1 4 1 1 3 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "r=TfidfVectorizer()\n",
        "term_freq=r.fit_transform([lemmatization_texts])\n",
        "print(term_freq.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaBbmwjxOZMQ",
        "outputId": "45a83a7d-c5eb-4a00-d245-d6724b48b345"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.13834289 0.06917145 0.13834289 0.06917145 0.06917145 0.06917145\n",
            "  0.13834289 0.06917145 0.20751434 0.06917145 0.13834289 0.06917145\n",
            "  0.20751434 0.06917145 0.06917145 0.06917145 0.55337157 0.06917145\n",
            "  0.06917145 0.06917145 0.06917145 0.06917145 0.06917145 0.20751434\n",
            "  0.06917145 0.06917145 0.06917145 0.06917145 0.06917145 0.20751434\n",
            "  0.06917145 0.06917145 0.06917145 0.06917145 0.06917145 0.06917145\n",
            "  0.13834289 0.06917145 0.06917145 0.06917145 0.27668579 0.06917145\n",
            "  0.06917145 0.20751434 0.13834289 0.06917145 0.06917145 0.06917145\n",
            "  0.13834289 0.06917145 0.06917145 0.06917145 0.06917145 0.06917145\n",
            "  0.06917145 0.06917145 0.06917145 0.06917145 0.06917145 0.06917145\n",
            "  0.06917145 0.06917145 0.13834289 0.06917145 0.06917145 0.06917145\n",
            "  0.06917145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedded Techniques\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "q6Uet5nGO2zk"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizes=['specific nlp process like automatic summarization — analyzing large volume text data producing executive summary — boon many industry including may considered “big data industries” big data process continue benefit improved nlp capability many data process translating information human language computer data processing translating computer data human language analysis decision making natural language processing continues become savvy big data capability become sophisticated getting started nlp talend nlp us various analysis lexical syntactic semantic pragmatic make possible computer read hear analyze languagebased data result technology chatbots able mimic human speech search engine able deliver accurate result users’ query']"
      ],
      "metadata": {
        "id": "chk1kT55P2aE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=100\n",
        "result=[one_hot(d,vocab_size) for d in lemmatizes]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z1YlLvpPSUd",
        "outputId": "6a32b5b3-b7b0-47c1-cbf2-a1f7dd223009"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[25, 95, 61, 11, 72, 75, 2, 83, 58, 52, 26, 74, 82, 72, 68, 2, 74, 29, 49, 47, 30, 10, 96, 74, 64, 12, 74, 61, 49, 7, 29, 95, 89, 29, 74, 61, 13, 18, 77, 6, 41, 74, 79, 13, 41, 74, 77, 6, 34, 7, 77, 41, 6, 79, 44, 79, 18, 12, 74, 89, 79, 93, 20, 91, 95, 48, 95, 40, 18, 34, 50, 9, 40, 5, 17, 27, 41, 77, 93, 1, 78, 74, 29, 71, 34, 76, 86, 77, 2, 86, 69, 76, 63, 71, 29, 37, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=8\n",
        "res=pad_sequences(result,maxlen=max_length,padding='post')\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-yBqNlbQEV0",
        "outputId": "236385a2-26ec-43e9-c4ff-06961095ad0f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[86 69 76 63 71 29 37  9]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}